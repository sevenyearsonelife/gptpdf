{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一张图片，展示了一位女士和一只狗在海滩上互动。他们似乎正在享受彼此的陪伴，并且看起来非常和谐。女士穿着格子衬衫，而狗狗则戴着项圈和牵引绳，表明它可能是一只宠物狗。背景是美丽的海景和日落时分的天空，为这个场景增添了一种宁静和平静的感觉。\n"
     ]
    }
   ],
   "source": [
    "from http import HTTPStatus\n",
    "import dashscope\n",
    "\n",
    "dashscope.api_key = \"sk-bcd2c1a8f3d84997b973883104376423\"\n",
    "model = \"qwen-vl-plus\"\n",
    "model = \"qwen-vl-max\"\n",
    "\n",
    "\n",
    "\n",
    "def simple_multimodal_conversation_call():\n",
    "    \"\"\"Simple single round multimodal conversation call.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"},\n",
    "                {\"text\": \"这是什么?\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    response = dashscope.MultiModalConversation.call(model=model,\n",
    "                                                     messages=messages)\n",
    "    # The response status_code is HTTPStatus.OK indicate success,\n",
    "    # otherwise indicate request is failed, you can get error code\n",
    "    # and message from code and message.\n",
    "    if response.status_code == HTTPStatus.OK:\n",
    "        print(response[\"output\"][\"choices\"][0][\"message\"][\"content\"][0][\"text\"])\n",
    "    else:\n",
    "        print(response.code)  # The error code.\n",
    "        print(response.message)  # The error message.\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    simple_multimodal_conversation_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse page: 0\n",
      "parse page: 1\n",
      "parse page: 2\n",
      "parse page: 3\n",
      "parse page: 4\n",
      "parse page: 5\n",
      "parse page: 6\n",
      "parse page: 7\n",
      "parse page: 8\n",
      "gpt parse page: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "ERROR:root:LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/agent/agent.py\", line 310, in _llm_and_parse_output\n",
      "    for token in response:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 117, in _llm_inference_with_stream\n",
      "    raise ValueError('LLM(Large Languate Model) error, Please check your key or base_url, or network')\n",
      "ValueError: LLM(Large Languate Model) error, Please check your key or base_url, or network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt parse page: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "ERROR:root:LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/agent/agent.py\", line 310, in _llm_and_parse_output\n",
      "    for token in response:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 117, in _llm_inference_with_stream\n",
      "    raise ValueError('LLM(Large Languate Model) error, Please check your key or base_url, or network')\n",
      "ValueError: LLM(Large Languate Model) error, Please check your key or base_url, or network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt parse page: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "ERROR:root:LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/agent/agent.py\", line 310, in _llm_and_parse_output\n",
      "    for token in response:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 117, in _llm_inference_with_stream\n",
      "    raise ValueError('LLM(Large Languate Model) error, Please check your key or base_url, or network')\n",
      "ValueError: LLM(Large Languate Model) error, Please check your key or base_url, or network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt parse page: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "ERROR:root:LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/agent/agent.py\", line 310, in _llm_and_parse_output\n",
      "    for token in response:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 117, in _llm_inference_with_stream\n",
      "    raise ValueError('LLM(Large Languate Model) error, Please check your key or base_url, or network')\n",
      "ValueError: LLM(Large Languate Model) error, Please check your key or base_url, or network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt parse page: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "ERROR:root:LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/agent/agent.py\", line 310, in _llm_and_parse_output\n",
      "    for token in response:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 117, in _llm_inference_with_stream\n",
      "    raise ValueError('LLM(Large Languate Model) error, Please check your key or base_url, or network')\n",
      "ValueError: LLM(Large Languate Model) error, Please check your key or base_url, or network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt parse page: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "ERROR:root:LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/agent/agent.py\", line 310, in _llm_and_parse_output\n",
      "    for token in response:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 117, in _llm_inference_with_stream\n",
      "    raise ValueError('LLM(Large Languate Model) error, Please check your key or base_url, or network')\n",
      "ValueError: LLM(Large Languate Model) error, Please check your key or base_url, or network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt parse page: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "ERROR:root:LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/agent/agent.py\", line 310, in _llm_and_parse_output\n",
      "    for token in response:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 117, in _llm_inference_with_stream\n",
      "    raise ValueError('LLM(Large Languate Model) error, Please check your key or base_url, or network')\n",
      "ValueError: LLM(Large Languate Model) error, Please check your key or base_url, or network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt parse page: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "ERROR:root:LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/agent/agent.py\", line 310, in _llm_and_parse_output\n",
      "    for token in response:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 117, in _llm_inference_with_stream\n",
      "    raise ValueError('LLM(Large Languate Model) error, Please check your key or base_url, or network')\n",
      "ValueError: LLM(Large Languate Model) error, Please check your key or base_url, or network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt parse page: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "ERROR:root:LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 108, in _llm_inference_with_stream\n",
      "    response = client.chat.completions.create(messages=messages, model=model, stream=True, temperature=temperature)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 643, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1250, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 931, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1030, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter', 'param': 'messages[0].content[1].image_url.url', 'message': \"'temperature' is not support for vl model now\", 'type': 'invalid_request_error'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/agent/agent.py\", line 310, in _llm_and_parse_output\n",
      "    for token in response:\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/GeneralAgent/skills/llm_inference.py\", line 117, in _llm_inference_with_stream\n",
      "    raise ValueError('LLM(Large Languate Model) error, Please check your key or base_url, or network')\n",
      "ValueError: LLM(Large Languate Model) error, Please check your key or base_url, or network\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "\n",
      "LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "\n",
      "LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "\n",
      "LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "\n",
      "LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "\n",
      "LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "\n",
      "LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "\n",
      "LLM(Large Languate Model) error, Please check your key or base_url, or network\n",
      "\n",
      "LLM(Large Languate Model) error, Please check your key or base_url, or network\n"
     ]
    }
   ],
   "source": [
    "from gptpdf import parse_pdf\n",
    "api_key = \"sk-bcd2c1a8f3d84997b973883104376423\"\n",
    "base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1/\"\n",
    "model = \"qwen-vl-max\"\n",
    "\n",
    "pdf_path = \"./bitcoin.pdf\"\n",
    "\n",
    "\n",
    "content, image_paths = parse_pdf(pdf_path, api_key=api_key, base_url=base_url, model=model)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GeneralAgent import Agent\n",
    "\n",
    "api_key = \"sk-bcd2c1a8f3d84997b973883104376423\"\n",
    "model = \"qwen-vl-max\"\n",
    "\n",
    "agent = Agent('你是一个AI助手', model=model, api_key=api_key)\n",
    "while True:\n",
    "    query = input('请输入: ')\n",
    "    agent.user_input(query)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
